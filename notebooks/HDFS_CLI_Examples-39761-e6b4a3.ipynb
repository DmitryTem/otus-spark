{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin   dev  home  lib32\tlibx32\tmnt\t  opt\troot  sbin  sys  usr\n",
      "boot  etc  lib\t lib64\tmedia\tnotebook  proc\trun   srv   tmp  var\n"
     ]
    }
   ],
   "source": [
    "! ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 08:50:28,777 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - jupyter supergroup          0 2024-11-18 08:01 /spark/jars\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 08:48:52,186 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 4 items\n",
      "drwxr-xr-x   - jupyter supergroup          0 2024-11-18 08:01 /spark\n",
      "drwxrwx---   - jupyter supergroup          0 2024-11-18 08:01 /tmp\n",
      "drwxr-xr-x   - jupyter supergroup          0 2024-11-18 08:02 /user\n",
      "drwxr-xr-x   - jupyter supergroup          0 2024-11-18 08:01 /var\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Hadoop is awesome!\" > example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop is awesome!\n"
     ]
    }
   ],
   "source": [
    "! cat example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put example.txt /tmp/hdfs-cli-example2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 jupyter supergroup         19 2024-11-26 08:11 /tmp/hdfs-cli-example2/example.txt\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop is awesome!\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /tmp/hdfs-cli-example2/example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cp /tmp/hdfs-cli-example2/example.txt /tmp/hdfs-cli-example2/example2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 jupyter supergroup         19 2024-11-26 08:11 /tmp/hdfs-cli-example2/example.txt\n",
      "-rw-r--r--   2 jupyter supergroup         19 2024-11-26 08:12 /tmp/hdfs-cli-example2/example2.txt\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -get /tmp/hdfs-cli-example2/example2.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " example2.txt  'HDFS CLI Examples.ipynb'   pyspark.ipynb   userdata.csv\n",
      " example.txt   'Hive Examples.ipynb'\t   test.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  38  /tmp/hdfs-cli-example2/example.txt\n",
      "19  38  /tmp/hdfs-cli-example2/example2.txt\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h /tmp/hdfs-cli-example2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://master:9870/fsck?ugi=jupyter&files=1&blocks=1&locations=1&path=%2Ftmp%2Fhdfs-cli-example2%2Fexample.txt\n",
      "FSCK started by jupyter (auth:SIMPLE) from /172.28.1.6 for path /tmp/hdfs-cli-example2/example.txt at Tue Nov 26 08:14:46 UTC 2024\n",
      "\n",
      "/tmp/hdfs-cli-example2/example.txt 19 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1018707957-172.28.1.2-1732569988465:blk_1073742088_1264 len=19 Live_repl=2  [DatanodeInfoWithStorage[172.28.1.3:9866,DS-90a23d41-ca0a-45ab-aaf0-0d6de73142ae,DISK], DatanodeInfoWithStorage[172.28.1.4:9866,DS-72d5d2a1-2d21-440f-8517-519bdcebd171,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t2\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t19 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 19 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Tue Nov 26 08:14:46 UTC 2024 in 12 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/tmp/hdfs-cli-example2/example.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "! hdfs fsck /tmp/hdfs-cli-example2/example.txt -files -blocks -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm example.txt example2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/hdfs-cli-example2\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\t\t  kms-log4j.properties\n",
      "configuration.xsl\t\t  kms-site.xml\n",
      "container-executor.cfg\t\t  log4j.properties\n",
      "core-site.xml\t\t\t  mapred-env.cmd\n",
      "hadoop-env.cmd\t\t\t  mapred-env.sh\n",
      "hadoop-env.sh\t\t\t  mapred-queues.xml.template\n",
      "hadoop-metrics2.properties\t  mapred-site.xml\n",
      "hadoop-policy.xml\t\t  shellprofile.d\n",
      "hadoop-user-functions.sh.example  ssl-client.xml.example\n",
      "hdfs-rbf-site.xml\t\t  ssl-server.xml.example\n",
      "hdfs-site.xml\t\t\t  user_ec_policies.xml.template\n",
      "httpfs-env.sh\t\t\t  workers\n",
      "httpfs-log4j.properties\t\t  yarn-env.cmd\n",
      "httpfs-site.xml\t\t\t  yarn-env.sh\n",
      "kms-acls.xml\t\t\t  yarnservice-log4j.properties\n",
      "kms-env.sh\t\t\t  yarn-site.xml\n"
     ]
    }
   ],
   "source": [
    "! ls /opt/hadoop/etc/hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    <property>\n",
      "        <name>dfs.namenode.name.dir</name>\n",
      "        <value>/opt/hadoop/dfs/name</value>\n",
      "    </property>\n",
      "    <property>\n",
      "        <name>dfs.datanode.data.dir</name>\n",
      "        <value>/opt/hadoop/dfs/data</value>\n",
      "    </property>\n",
      "    <property>\n",
      "        <name>dfs.namenode.checkpoint.dir</name>\n",
      "        <value>/opt/hadoop/dfs/namesecondary</value>\n",
      "    </property>\n",
      "    <!-- Replication factor. Files you store on HDFS will be stored\n",
      "    on this many machines -->\n",
      "    <property>\n",
      "        <name>dfs.replication</name>\n",
      "        <value>2</value>\n",
      "    </property>\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "! cat /opt/hadoop/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem        Size     Used  Available  Use%\n",
      "hdfs://master  116.7 G  644.0 M     17.7 G    1%\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "fsImageValidation    run FsImageValidation to check an fsimage\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshot           list all snapshots for a snapshottable directory\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "! hdfs -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
