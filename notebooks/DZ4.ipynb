{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c7d794-fbfa-40fd-bbdc-3d6bd8aaa7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad311abe-8973-45aa-bb70-d038514057c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/04 13:25:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0adcf14-1676-44ba-841a-2ea07dc0f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://metastore:5432/metastore\") \\\n",
    "    .option(\"dbtable\", \"information_schema.tables\") \\\n",
    "    .option(\"user\", \"jupyter\") \\\n",
    "    .option(\"password\", \"jupyter\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9535dc0b-593e-4a8b-be25-faa3c3e21bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+----------+----------+----------------------------+--------------------+-------------------------+------------------------+----------------------+------------------+--------+-------------+\n",
      "|table_catalog|table_schema|table_name|table_type|self_referencing_column_name|reference_generation|user_defined_type_catalog|user_defined_type_schema|user_defined_type_name|is_insertable_into|is_typed|commit_action|\n",
      "+-------------+------------+----------+----------+----------------------------+--------------------+-------------------------+------------------------+----------------------+------------------+--------+-------------+\n",
      "|    metastore|      public|       DBS|BASE TABLE|                        null|                null|                     null|                    null|                  null|               YES|      NO|         null|\n",
      "|    metastore|      public|  DB_PRIVS|BASE TABLE|                        null|                null|                     null|                    null|                  null|               YES|      NO|         null|\n",
      "|    metastore|      public|      IDXS|BASE TABLE|                        null|                null|                     null|                    null|                  null|               YES|      NO|         null|\n",
      "+-------------+------------+----------+----------+----------------------------+--------------------+-------------------------+------------------------+----------------------+------------------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ce079a-66f7-4127-855f-c07972a36074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99412049-0285-4258-8ab9-d9f02cb88b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.createOrReplaceTempView('df_table_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c61653-b46c-4bee-a855-b3fb76e37d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_table.write.format(\"orc\").saveAsTable('df_table_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ff92ed-e4c6-4004-a480-735011b33b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = d7bb266f-9048-49f7-8659-4f4f8c28c31d\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n",
      "Hive Session ID = cffd182d-669c-4f2b-9a14-b673544abb90\n",
      "OK\n",
      "CREATE TABLE `df_table_schema`(\n",
      "  `table_catalog` string, \n",
      "  `table_schema` string, \n",
      "  `table_name` string, \n",
      "  `table_type` string, \n",
      "  `self_referencing_column_name` string, \n",
      "  `reference_generation` string, \n",
      "  `user_defined_type_catalog` string, \n",
      "  `user_defined_type_schema` string, \n",
      "  `user_defined_type_name` string, \n",
      "  `is_insertable_into` string, \n",
      "  `is_typed` string, \n",
      "  `commit_action` string)\n",
      "ROW FORMAT SERDE \n",
      "  'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \n",
      "WITH SERDEPROPERTIES ( \n",
      "  'path'='hdfs://master/user/hive/warehouse/df_table_schema') \n",
      "STORED AS INPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' \n",
      "OUTPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\n",
      "LOCATION\n",
      "  'hdfs://master/user/hive/warehouse/df_table_schema'\n",
      "TBLPROPERTIES (\n",
      "  'spark.sql.create.version'='3.4.4', \n",
      "  'spark.sql.sources.provider'='orc', \n",
      "  'spark.sql.sources.schema'='{\"type\":\"struct\",\"fields\":[{\"name\":\"table_catalog\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"table_schema\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"table_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"table_type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"self_referencing_column_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"reference_generation\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"user_defined_type_catalog\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"user_defined_type_schema\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"user_defined_type_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"is_insertable_into\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"is_typed\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}},{\"name\":\"commit_action\",\"type\":\"string\",\"nullable\":true,\"metadata\":{\"scale\":0}}]}', \n",
      "  'transient_lastDdlTime'='1735997132')\n",
      "Time taken: 0.787 seconds, Fetched: 28 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -e \"show create table df_table_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc749a6e-13b3-4f11-8616-31c1e7eafbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = 9de35d91-43f5-4049-8ebe-33ed3d87d0da\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n",
      "Hive Session ID = ec0212b4-8d16-4048-81cd-0d8e8ffcf59f\n",
      "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.default.tables already exists)\n"
     ]
    }
   ],
   "source": [
    "! hive -e \" CREATE TABLE tables ( \\\n",
    "   table_catalog string, \\\n",
    "   table_schema  string, \\\n",
    "   table_name  string, \\\n",
    "   table_type  string, \\\n",
    "   self_referencing_column_name  string, \\\n",
    "   reference_generation  string, \\\n",
    "   user_defined_type_catalog  string, \\\n",
    "   user_defined_type_schema  string, \\\n",
    "   user_defined_type_name  string, \\\n",
    "   is_insertable_into  string, \\\n",
    "   is_typed  string, \\\n",
    "   commit_action  string) \\\n",
    "ROW FORMAT SERDE \\\n",
    "  'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \\\n",
    "LOCATION \\\n",
    "  'hdfs://master/user/hive/warehouse/tables' \\\n",
    "  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164672ea-7b6e-4aa3-9543-20de25a00898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = 357ab0d8-af43-4c79-ba78-3c347d8e4f28\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true\n",
      "Hive Session ID = 18446d19-1a62-4042-9205-df18cff64f53\n",
      "Query ID = jupyter_20250104133618_62d8970a-f012-4c1e-aabb-dbb7cafabcc9\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1735917189604_0008, Tracking URL = http://master:8088/proxy/application_1735917189604_0008/\n",
      "Kill Command = /opt/hadoop/bin/mapred job  -kill job_1735917189604_0008\n",
      "^C\n",
      "killing job with: job_1735917189604_0008\n"
     ]
    }
   ],
   "source": [
    "! hive -e \"insert into tables (select * from df_table_v)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9246960-2742-428a-921a-bf9d09b3ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('insert into tables (select * from df_table_v)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c57f4c3-c6c4-4a6d-b325-9f86ec9cf05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+----------+----------+----------------------------+--------------------+-------------------------+------------------------+----------------------+------------------+--------+-------------+\n",
      "|table_catalog|table_schema|table_name|table_type|self_referencing_column_name|reference_generation|user_defined_type_catalog|user_defined_type_schema|user_defined_type_name|is_insertable_into|is_typed|commit_action|\n",
      "+-------------+------------+----------+----------+----------------------------+--------------------+-------------------------+------------------------+----------------------+------------------+--------+-------------+\n",
      "|    metastore|      public|       DBS|BASE TABLE|                        null|                null|                     null|                    null|                  null|               YES|      NO|         null|\n",
      "|    metastore|      public|  DB_PRIVS|BASE TABLE|                        null|                null|                     null|                    null|                  null|               YES|      NO|         null|\n",
      "|    metastore|      public|      IDXS|BASE TABLE|                        null|                null|                     null|                    null|                  null|               YES|      NO|         null|\n",
      "+-------------+------------+----------+----------+----------------------------+--------------------+-------------------------+------------------------+----------------------+------------------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from tables').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3512e6a-17df-479c-a891-7ac1a9866c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
